{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resource Generation Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------------------\n",
        "# imports\n",
        "#--------------------------------------\n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from shutil import copy\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------------------\n",
        "# string resources\n",
        "#--------------------------------------\n",
        "numbers                 =       ['০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯']\n",
        "punctuations            =       ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '।']\n",
        "vowelDiacritic          =       ['ꠣ', 'ꠤ', 'ꠥ' , 'ꠦ' , 'ꠂ' , 'ꠧ']\n",
        "consonantDiacritic      =       ['ꠋ', '꠬']\n",
        "consonants              =       ['ꠇ', 'ꠈ', 'ꠉ', 'ꠊ', 'ꠌ', 'ꠍ', 'ꠎ', 'ꠏ', 'ꠐ', 'ꠑ', 'ꠒ', 'ꠓ', 'ꠘ', 'ꠔ', 'ꠕ', 'ꠖ', \n",
        "                                'ꠗ', 'ꠙ', 'ꠚ', 'ꠛ', 'ꠜ', 'ꠝ', 'ꠎ', 'ꠞ', 'ꠟ', 'ꠡ', 'ꠢ', 'ꠠ', 'ꠅ']\n",
        "vowels                  =       ['ꠅ', 'ꠀ',  'ꠁ', 'ꠃ', 'ꠄ', 'ꠀꠄ', 'ꠅ꠆ꠎꠣ'] \n",
        "vowelConsonantConjunct  =       ['ꠀꠔ', 'ꠀꠞ', 'ꠀꠟ', 'ꠀꠡ']\n",
        "consonant_conjunct1     =       ['ꠇ꠆ꠇ','ꠇ꠆ꠔ','ꠌ꠆ꠌ','ꠌ꠆ꠍ','ꠎ꠆ꠎ','ꠔ꠆ꠔ','ꠘ꠆ꠔ','ꠘ꠆ꠖ','ꠘ꠆ꠎ','ꠘ꠆ꠘ','ꠛ꠆ꠛ','ꠝ꠆ꠛ','ꠝ꠆ꠝ','ꠞꠟ','ꠟ꠆ꠟ','ꠡ꠆ꠇ','ꠡꠌ','ꠡꠍ','ꠡ꠆ꠐ','ꠡꠑ','ꠡ꠆ꠛ']\n",
        "consonant_conjunct2     =       ['ꠇ꠆ꠞ','ꠇ꠆ꠐ' ,'ꠇ꠆ꠟ','ꠇ꠆ꠡ','ꠈ꠆ꠔ','ꠉ꠆ꠉ','ꠎ꠆ꠘ','ꠉ꠆ꠘ','ꠉ꠆ꠞ','ꠉ꠆ꠟ','ꠐ꠆ꠐ','ꠐ꠆ꠞ','ꠒ꠆ꠒ','ꠒ꠆ꠞ','ꠔ꠆ꠞ','ꠖ꠆ꠖ','ꠖ꠆ꠞ','ꠘ꠆ꠇ','ꠘ꠆ꠌ',\n",
        "                                'ꠘ꠆ꠍ','ꠘ꠆ꠐ','ꠘ꠆ꠒ','ꠘ꠆ꠗ','ꠘ꠆ꠡ','ꠙ꠆ꠐ','ꠙ꠆ꠔ','ꠙ꠆ꠙ' ,'ꠙ꠆ꠞ' ,'ꠙ꠆ꠟ','ꠚ꠆ꠞ','ꠛ꠆ꠞ' ,'ꠛ꠆ꠟ' ,'ꠝ꠆ꠙ','ꠝ꠆ꠞ','ꠞ꠆ꠛ','ꠟ꠆ꠐ','ꠟ꠆ꠙ','ꠡ꠆ꠐ',\n",
        "                                'ꠡ꠆ꠕ','ꠡ꠆ꠞ','ꠡꠡ','ꠢ꠆ꠞ']\n",
        "\n",
        "\n",
        "\n",
        "consonant_conjunct = ['ꠀꠔ', 'ꠀꠞ', 'ꠀꠟ', 'ꠀꠡ', 'ꠄ꠆ꠎ', 'ꠅ꠆ꠌ', 'ꠅ꠆ꠍ', 'ꠅ꠆ꠎ', 'ꠅ꠆ꠏ', 'ꠇ', 'ꠇ꠆ꠇ', 'ꠇ꠆ꠎ', 'ꠇ꠆ꠐ', 'ꠇ꠆ꠐ ', 'ꠇ꠆ꠐ꠆ꠎ', 'ꠇ꠆ꠐ꠆ꠞ', 'ꠇ꠆ꠔ', 'ꠇ꠆ꠔ꠆ꠞ', \n",
        "                    'ꠇ꠆ꠛ', 'ꠇ꠆ꠝ', 'ꠇ꠆ꠞ', 'ꠇ꠆ꠞ꠆ꠎ', 'ꠇ꠆ꠟ', 'ꠇ꠆ꠟ꠆ꠎ', 'ꠇ꠆ꠡ', 'ꠇ꠆ꠡ꠆ꠎ', 'ꠇ꠆ꠡ꠆ꠘ', 'ꠇ꠆ꠡ꠆ꠛ', 'ꠇ꠆ꠡ꠆ꠝ', 'ꠇ꠆ꠡ꠆ꠝ꠆ꠎ', 'ꠈ꠆ꠎ', 'ꠈ꠆ꠔ', 'ꠈ꠆ꠞ', 'ꠈꠔ', 'ꠉ꠆ꠉ', \n",
        "                    'ꠉ꠆ꠎ', 'ꠉ꠆ꠗ', 'ꠉ꠆ꠗ꠆ꠎ', 'ꠉ꠆ꠗ꠆ꠞ', 'ꠉ꠆ꠘ', 'ꠉ꠆ꠘ꠆ꠎ', 'ꠉ꠆ꠛ', 'ꠉ꠆ꠝ', 'ꠉ꠆ꠞ', 'ꠉ꠆ꠞ꠆ꠎ', 'ꠉ꠆ꠟ', 'ꠉ꠆ꠟ꠆ꠎ', 'ꠉꠉ', 'ꠊ꠆ꠎ', 'ꠊ꠆ꠘ', 'ꠊ꠆ꠞ', 'ꠌ꠆ꠅ', 'ꠌ꠆ꠌ', \n",
        "                    'ꠌ꠆ꠍ', 'ꠌ꠆ꠍ꠆ꠛ', 'ꠌ꠆ꠍ꠆ꠞ', 'ꠌ꠆ꠎ', 'ꠌ꠆ꠛ', 'ꠎ꠆ꠅ', 'ꠎ꠆ꠎ', 'ꠎ꠆ꠎ꠆ꠛ', 'ꠎ꠆ꠏ', 'ꠎ꠆ꠘ', 'ꠎ꠆ꠛ', 'ꠎ꠆ꠞ', 'ꠐ꠆ꠎ', 'ꠐ꠆ꠐ', 'ꠐ꠆ꠛ', 'ꠐ꠆ꠝ', 'ꠐ꠆ꠞ', 'ꠐ꠆ꠞ꠆ꠎ', 'ꠑ꠆ꠎ', \n",
        "                    'ꠒ꠆ꠎ', 'ꠒ꠆ꠒ', 'ꠒ꠆ꠛ', 'ꠒ꠆ꠞ', 'ꠒ꠆ꠞ꠆ꠎ', 'ꠓ꠆ꠎ', 'ꠓ꠆ꠞ', 'ꠔ꠆ꠎ', 'ꠔ꠆ꠔ', 'ꠔ꠆', 'ꠔ꠆ꠔ꠆ꠎ', 'ꠔ꠆ꠔ꠆ꠛ', 'ꠔ꠆ꠕ', 'ꠔ꠆ꠘ', 'ꠔ꠆ꠛ', 'ꠔ꠆ꠝ', 'ꠔ꠆ꠝ꠆ꠎ', 'ꠔ꠆ꠞ', \n",
        "                    'ꠔ꠆ꠞ꠆ꠎ', 'ꠕ꠆ꠎ', 'ꠕ꠆ꠛ', 'ꠕ꠆ꠞ', 'ꠕ꠆ꠞ꠆ꠎ', 'ꠖ꠆ꠉ', 'ꠖ꠆ꠊ', 'ꠖ꠆ꠎ', 'ꠖ꠆ꠖ', 'ꠖ꠆ꠖ꠆ꠛ', 'ꠖ꠆ꠗ', 'ꠖ꠆ꠛ', 'ꠖ꠆ꠜ', 'ꠖ꠆ꠜ꠆ꠞ', 'ꠖ꠆ꠝ', 'ꠖ꠆ꠞ', 'ꠖ꠆ꠞ꠆ꠎ', \n",
        "                    'ꠗ꠆ꠎ', 'ꠗ꠆ꠘ', 'ꠗ꠆ꠛ', 'ꠗ꠆ꠝ', 'ꠗ꠆ꠞ', 'ꠘ꠆ꠇ', 'ꠘ꠆ꠌ', 'ꠘ꠆ꠍ', 'ꠘ꠆ꠎ', 'ꠘ꠆ꠐ', 'ꠘ꠆ꠐ꠆ꠎ', 'ꠘ꠆ꠐ꠆ꠞ', 'ꠘ꠆ꠐ꠆ꠞ꠆ꠎ', 'ꠘ꠆ꠑ', 'ꠘ꠆ꠑ꠆ꠎ', 'ꠘ꠆ꠒ', 'ꠘ꠆ꠒ꠆ꠎ', 'ꠘ꠆ꠒ꠆ꠛ', \n",
        "                    'ꠘ꠆ꠒ꠆ꠞ', 'ꠘ꠆ꠓ', 'ꠘ꠆ꠔ', 'ꠘ꠆ꠔ', 'ꠘ꠆ꠔ꠆ꠎ', 'ꠘ꠆ꠔ꠆ꠛ', 'ꠘ꠆ꠔ꠆ꠞ', 'ꠘ꠆ꠔ꠆ꠞ꠆ꠎ', 'ꠘ꠆ꠕ', 'ꠘ꠆ꠕ꠆ꠎ', 'ꠘ꠆ꠕ꠆ꠞ', 'ꠘ꠆ꠖ', 'ꠘ꠆ꠖ', 'ꠘ꠆ꠖ꠆ꠎ', 'ꠘ꠆ꠖ꠆ꠛ', 'ꠘ꠆ꠖ꠆ꠞ', 'ꠘ꠆ꠗ', \n",
        "                    'ꠘ꠆ꠗ꠆ꠎ', 'ꠘ꠆ꠗ꠆ꠞ', 'ꠘ꠆ꠘ', 'ꠘ꠆ꠘ', 'ꠘ꠆ꠛ', 'ꠘ꠆ꠝ', 'ꠘ꠆ꠡ', 'ꠘ꠆ꠡ꠆ꠎ', 'ꠙ꠆ꠎ', 'ꠙ꠆ꠐ', 'ꠙ꠆ꠐ꠆ꠎ', 'ꠙ꠆ꠔ', 'ꠙ꠆ꠘ', 'ꠙ꠆ꠙ', 'ꠙ꠆ꠙ ', 'ꠙ꠆ꠞ', 'ꠙ꠆ꠞ ', \n",
        "                    'ꠙ꠆ꠞ꠆ꠎ', 'ꠙ꠆ꠟ', 'ꠙ꠆ꠟ꠆ꠎ', 'ꠙ꠆ꠡ', 'ꠚ꠆ꠎ', 'ꠚ꠆ꠞ', 'ꠚ꠆ꠞ꠆ꠎ', 'ꠚ꠆ꠟ', 'ꠚ꠆ꠟ꠆ꠎ', 'ꠛ꠆ꠎ', 'ꠛ꠆ꠖ', 'ꠛ꠆ꠗ', 'ꠛ꠆ꠛ', 'ꠛ꠆ꠛ', 'ꠛ꠆ꠞ', 'ꠛ꠆ꠞ ', 'ꠛ꠆ꠞ꠆ꠎ', \n",
        "                    'ꠛ꠆ꠟ', 'ꠛ꠆ꠟ ', 'ꠛ꠆ꠟ꠆ꠎ', 'ꠜ꠆ꠎ', 'ꠜ꠆ꠛ', 'ꠜ꠆ꠞ', 'ꠜ꠆ꠟ', 'ꠝ꠆ꠎ', 'ꠝ꠆ꠘ', 'ꠝ꠆ꠘ꠆ꠎ', 'ꠝ꠆ꠙ', 'ꠝ꠆ꠙ꠆ꠎ', 'ꠝ꠆ꠙ꠆ꠞ', 'ꠝ꠆ꠚ', 'ꠝ꠆ꠛ', 'ꠝ꠆ꠛ꠆ꠞ', 'ꠝ꠆ꠜ', 'ꠝ꠆ꠜ꠆ꠞ', \n",
        "                    'ꠝ꠆ꠝ', 'ꠝ꠆ꠝ ', 'ꠝ꠆ꠞ', 'ꠝ꠆ꠟ', 'ꠞ꠆ꠇ', 'ꠞ꠆ꠇ꠆ꠎ', 'ꠞ꠆ꠇ꠆ꠐ', 'ꠞ꠆ꠈ', 'ꠞ꠆ꠉ', 'ꠞ꠆ꠉ꠆ꠎ', 'ꠞ꠆ꠉ꠆ꠞ', 'ꠞ꠆ꠊ', 'ꠞ꠆ꠊ꠆ꠎ', 'ꠞ꠆ꠌ', 'ꠞ꠆ꠌ꠆ꠎ', 'ꠞ꠆ꠍ', 'ꠞ꠆ꠎ', \n",
        "                    'ꠞ꠆ꠎ꠆ꠅ', 'ꠞ꠆ꠎ꠆ꠎ', 'ꠞ꠆ꠏ', 'ꠞ꠆ꠐ', 'ꠞ꠆ꠐ꠆ꠎ', 'ꠞ꠆ꠐ꠆ꠞ', 'ꠞ꠆ꠒ', 'ꠞ꠆ꠒ꠆ꠞ', 'ꠞ꠆ꠓ꠆ꠎ', 'ꠞ꠆ꠔ', 'ꠞ꠆ꠔ꠆ꠎ', 'ꠞ꠆ꠔ꠆ꠝ', 'ꠞ꠆ꠔ꠆ꠞ', 'ꠞ꠆ꠕ', 'ꠞ꠆ꠕ꠆ꠎ', 'ꠞ꠆ꠖ', 'ꠞ꠆ꠖ꠆ꠛ', \n",
        "                    'ꠞ꠆ꠖ꠆ꠞ', 'ꠞ꠆ꠗ', 'ꠞ꠆ꠗ꠆ꠛ', 'ꠞ꠆ꠘ', 'ꠞ꠆ꠘ꠆ꠎ', 'ꠞ꠆ꠘ꠆ꠒ', 'ꠞ꠆ꠘ꠆ꠔ', 'ꠞ꠆ꠙ', 'ꠞ꠆ꠙ꠆ꠐ', 'ꠞ꠆ꠙ꠆ꠟ', 'ꠞ꠆ꠚ', 'ꠞ꠆ꠛ', 'ꠞ꠆ꠛ꠆ꠎ', 'ꠞ꠆ꠜ', 'ꠞ꠆ꠝ', 'ꠞ꠆ꠝ꠆ꠎ', 'ꠞ꠆ꠝ꠆ꠕ', \n",
        "                    'ꠞ꠆ꠝ꠆ꠙ', 'ꠞ꠆ꠟ', 'ꠞ꠆ꠟ꠆ꠎ', 'ꠞ꠆ꠟ꠆ꠒ', 'ꠞ꠆ꠡ', 'ꠞ꠆ꠡ꠆ꠎ', 'ꠞ꠆ꠡ꠆ꠐ', 'ꠞ꠆ꠡ꠆ꠛ', 'ꠞ꠆ꠡ꠆ꠝ', 'ꠞ꠆ꠢ', 'ꠞ꠆ꠢ꠆ꠎ', 'ꠞꠟ', 'ꠟ꠆ꠇ', 'ꠟ꠆ꠇ꠆ꠎ', 'ꠟ꠆ꠉ', 'ꠟ꠆ꠌ', 'ꠟ꠆ꠎ', \n",
        "                    'ꠟ꠆ꠐ', 'ꠟ꠆ꠐ꠆ꠎ', 'ꠟ꠆ꠐ꠆ꠞ', 'ꠟ꠆ꠒ', 'ꠟ꠆ꠒ꠆ꠎ', 'ꠟ꠆ꠒ꠆ꠞ', 'ꠟ꠆ꠙ', 'ꠟ꠆ꠚ', 'ꠟ꠆ꠛ', 'ꠟ꠆ꠛ꠆ꠎ', 'ꠟ꠆ꠜ', 'ꠟ꠆ꠝ', 'ꠟ꠆ꠟ', 'ꠡ꠆ꠇ', 'ꠡ꠆ꠇ ', 'ꠡ꠆ꠇ꠆ꠎ', 'ꠡ꠆ꠇ꠆ꠞ', \n",
        "                    'ꠡ꠆ꠇ꠆ꠞ꠆ꠎ', 'ꠡ꠆ꠈ', 'ꠡ꠆ꠌ', 'ꠡ꠆ꠍ', 'ꠡ꠆ꠎ', 'ꠡ꠆ꠐ', 'ꠡ꠆ꠐ꠆ꠎ', 'ꠡ꠆ꠐ꠆ꠞ', 'ꠡ꠆ꠐ꠆ꠞ꠆ꠎ', 'ꠡ꠆ꠑ', 'ꠡ꠆ꠑ꠆ꠎ', 'ꠡ꠆ꠔ', 'ꠡ꠆ꠔ꠆ꠎ', 'ꠡ꠆ꠔ꠆ꠛ', 'ꠡ꠆ꠔ꠆ꠞ', 'ꠡ꠆ꠕ', 'ꠡ꠆ꠕ꠆ꠎ', \n",
        "                    'ꠡ꠆ꠘ', 'ꠡ꠆ꠘ꠆ꠎ', 'ꠡ꠆ꠙ', 'ꠡ꠆ꠙ꠆ꠎ', 'ꠡ꠆ꠙ꠆ꠞ', 'ꠡ꠆ꠙ꠆ꠞ꠆ꠎ', 'ꠡ꠆ꠙ꠆ꠟ', 'ꠡ꠆ꠙ꠆ꠟ꠆ꠎ', 'ꠡ꠆ꠚ', 'ꠡ꠆ꠛ', 'ꠡ꠆ꠝ', 'ꠡ꠆ꠝ꠆ꠎ', 'ꠡ꠆ꠞ', 'ꠡ꠆ꠞ꠆ꠎ', 'ꠡ꠆ꠟ', 'ꠡ꠆ꠟ꠆ꠎ', \n",
        "                    'ꠡꠌ', 'ꠡꠍ', 'ꠡꠑ', 'ꠡꠡ', 'ꠢ꠆ꠎ', 'ꠢ꠆ꠘ', 'ꠢ꠆ꠛ', 'ꠢ꠆ꠝ', 'ꠢ꠆ꠞ', 'ꠢ꠆ꠟ']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic Grapheme based dictionary creation\n",
        "* **NUM_MIX_DATA**: The amount of data to create where numbers,punctuations and graphemes are mixed at random\n",
        "* **NUM_NUM_DATA**: The amount of numeric data to create \n",
        "* **NUM_GPM_LOOP**: Number of loop to go over the grapheme list. In each loop , the whole set of grapheme are covered at-least once.\n",
        "* **DICT_NAME**   : The path to save the dictionary (word based)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_MIX_DATA= 300000\n",
        "NUM_NUM_DATA= 100000\n",
        "NUM_GPM_LOOP= 5000\n",
        "DICT_NAME   = '/home/apsisdev/OCR/SylhetiNagri/dicts/synthdict.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGZzYM_gW-rb",
        "outputId": "13310594-1bc5-4a46-f630-6ff00ccd26a4"
      },
      "outputs": [],
      "source": [
        "#-------------------------------\n",
        "# creating graphemes\n",
        "#-------------------------------\n",
        "graphemes = vowels + consonants + consonant_conjunct + \\\n",
        "            [x+ consonantDiacritic[0] for x in vowels+consonants+consonant_conjunct]+ \\\n",
        "            [x+ consonantDiacritic[1] for x in vowels+consonants+consonant_conjunct] + \\\n",
        "            [x+ consonantDiacritic[0]+consonantDiacritic[1] for x in vowels+consonants+consonant_conjunct]\n",
        "for v in vowelDiacritic:\n",
        "    for c in consonants+consonant_conjunct:\n",
        "        graphemes.append(c+v)\n",
        "        graphemes.append(c+v+consonantDiacritic[0])\n",
        "        \n",
        "graphemes= list(np.unique([x.replace(' ', '').replace('ꠋ'+'꠬','ꠋ') for x in graphemes]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CvDByAp8XBh_"
      },
      "outputs": [],
      "source": [
        "def random_exec(poplutation=[0,1],weights=[0.5,0.5],match=0):\n",
        "    return random.choices(population=poplutation,weights=weights,k=1)[0]==match\n",
        "\n",
        "def create_words(graphemes,\n",
        "                min_len=1,\n",
        "                max_len=10,\n",
        "                mods=[],#['ঁ', 'ং', 'ঃ'],\n",
        "                mod_weights=[0.3,0.7]):\n",
        "    \n",
        "    _graphemes = graphemes.copy()\n",
        "    random.shuffle(_graphemes)\n",
        "    words = [] \n",
        "    index = 0 \n",
        "    length = len(_graphemes) \n",
        "    while (index < length):\n",
        "        _len = random.randint(min_len,max_len)\n",
        "        word=_graphemes[index:index+_len]\n",
        "        if random_exec(weights=mod_weights):\n",
        "            wlen=len(word)\n",
        "            widx=random.randint(0,wlen-1)\n",
        "            #word[widx]+=random.choice(mods) \n",
        "        words.append(\"\".join(word)) \n",
        "        index = index + _len\n",
        "    return words\n",
        "def create_numbers(numbers,\n",
        "                min_len=1,\n",
        "                max_len=10,\n",
        "                num_samples=1000000):\n",
        "    \n",
        "    words = [] \n",
        "    for _ in range(num_samples):\n",
        "        _len = random.randint(min_len,max_len)\n",
        "        _word=[]\n",
        "        for _ in range(_len):_word.append(random.choice(numbers))\n",
        "        if random_exec():_word[random.randint(0,_len-1)]+=\".\"\n",
        "        words.append(\"\".join(_word))\n",
        "    return words\n",
        "\n",
        "def create_mixed_data(numbers,\n",
        "                    graphemes,\n",
        "                    punctuations,    \n",
        "                    num_samples=100000,\n",
        "                    lens= [1,2,3,4,5,6,7,8,9,10],\n",
        "                    weights= [0.05,0.05,0.1,0.15,0.15,0.15,0.15,0.1,0.05,0.05],\n",
        "                    comp_weights= [0.33,0.34,0.33]):\n",
        "    words=[]\n",
        "    for _ in tqdm(range(num_samples)):\n",
        "        len_word=random.choices(population=lens,weights=weights,k=1)[0]\n",
        "        _graphemes=[]\n",
        "        for _ in range(len_word):\n",
        "            _ctype=random.choices(population=[\"g\",\"n\",\"p\"],weights=comp_weights,k=1)[0]\n",
        "            if _ctype==\"g\":    \n",
        "                _graphemes.append(random.choice(graphemes))\n",
        "            elif _ctype==\"n\":    \n",
        "                _graphemes.append(random.choice(numbers))\n",
        "            else:\n",
        "                _graphemes.append(random.choice(punctuations))        \n",
        "        words.append(\"\".join(_graphemes))\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyA16YHJXJ7z",
        "outputId": "102b878d-2894-4324-a366-3917933242bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300000/300000 [00:02<00:00, 122157.86it/s]\n",
            "100%|██████████| 5000/5000 [00:13<00:00, 357.31it/s]\n",
            "100%|██████████| 5025556/5025556 [01:25<00:00, 58619.96it/s]\n"
          ]
        }
      ],
      "source": [
        "words=create_mixed_data(numbers,graphemes,punctuations,num_samples=NUM_MIX_DATA)\n",
        "dfm=pd.DataFrame({\"word\":words})\n",
        "\n",
        "words=create_numbers(numbers,num_samples=NUM_NUM_DATA)\n",
        "dfn=pd.DataFrame({\"word\":words})\n",
        "\n",
        "gwords=[]\n",
        "for i in tqdm(range(NUM_GPM_LOOP)):\n",
        "    gwords+=create_words(graphemes)\n",
        "dfg=pd.DataFrame({\"word\":gwords})\n",
        "\n",
        "dfs=[dfm,dfn,dfg]\n",
        "\n",
        "df=pd.concat(dfs,ignore_index=True)\n",
        "df=df.sample(frac=1)\n",
        "\n",
        "with open(DICT_NAME,\"w+\") as f:\n",
        "    for idx in tqdm(range(len(df))):\n",
        "        word=df.iloc[idx,0]\n",
        "        f.write(f\"{word}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vocab creation\n",
        "* **SINGLE_LINE_VOCAB_TXT**: file path for creating (mostly) synthtiger format vocabulary/charset \n",
        "\n",
        "    ```text \n",
        "    012345abcd....\n",
        "    ```\n",
        "* **MULTI_LINE_VOCAB_TXT** : file path for creating (mostly) synthindic format line separated vocab/charset\n",
        "\n",
        "    ```text\n",
        "    0\n",
        "    1\n",
        "    2\n",
        "    a\n",
        "    b\n",
        "    c\n",
        "    .\n",
        "    .\n",
        "\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "SINGLE_LINE_VOCAB_TXT=\"/home/apsisdev/OCR/SylhetiNagri/vocabs/charset.txt\"\n",
        "MULTI_LINE_VOCAB_TXT =\"/home/apsisdev/OCR/SylhetiNagri/vocabs/vocab.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "unicodes=[]\n",
        "\n",
        "for comp in vowels+consonants+consonantDiacritic+vowelDiacritic+punctuations+numbers:\n",
        "    for u in comp:\n",
        "        if u not in unicodes:\n",
        "            unicodes.append(u)\n",
        "    \n",
        "unicodes=sorted(list(set(unicodes)))\n",
        "# synthindic\n",
        "with open(MULTI_LINE_VOCAB_TXT,\"w+\") as f:\n",
        "    for u in unicodes:\n",
        "        f.write(f\"{u}\\n\")\n",
        "# synthtiger\n",
        "charset=\"\".join(unicodes)\n",
        "with open(SINGLE_LINE_VOCAB_TXT,\"w+\") as f:\n",
        "    f.write(charset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train set words\n",
        "* **CSV_PATH** = The csv that contains the following columns: ```['filename', 'SN', 'source', 'fold']```\n",
        "* **DICT_NAME**= The path to save dictionary created form the unique words in the train-set   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_PATH =\"/home/apsisdev/OCR/SylhetiNagri/datasets/SNdataset/SN_OCR.csv\"\n",
        "DICT_NAME=\"/home/apsisdev/OCR/SylhetiNagri/dicts/traindict.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data unique words: 1573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1573/1573 [00:00<00:00, 1853269.72it/s]\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(CSV_PATH)\n",
        "train=df.loc[df.fold==\"train\"]\n",
        "words=[w for w in train.SN.unique()]\n",
        "print(\"Train data unique words:\",len(words))\n",
        "with open(DICT_NAME,\"w+\") as f:\n",
        "    for word in tqdm(words):\n",
        "        f.write(f\"{word}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Separation\n",
        "* **DATASET_PATH**: The path where train and test folders will be created/ the path to save the separated data\n",
        "* **IMAGE_FOLDER_PATH**: The path where all the images are available. Namely *data* folder from [here](/home/apsisdev/OCR/SylhetiNagri/datasets/SNdataset/data) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_PATH=\"/home/apsisdev/OCR/SylhetiNagri/datasets/\" \n",
        "IMAGE_FOLDER_PATH=\"/home/apsisdev/OCR/SylhetiNagri/datasets/SNdataset/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4305/4305 [00:00<00:00, 837227.19it/s]\n"
          ]
        }
      ],
      "source": [
        "df[\"filename\"]=df[\"filename\"].progress_apply(lambda x:os.path.join(IMAGE_FOLDER_PATH,x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------\n",
        "# functions\n",
        "#---------------------------------------\n",
        "def create_dir(base,ext):\n",
        "    _path=os.path.join(base,ext)\n",
        "    if not os.path.exists(_path):\n",
        "        os.mkdir(_path)\n",
        "    return _path\n",
        "\n",
        "#---------------------------------------\n",
        "# directories of folders and files\n",
        "#---------------------------------------\n",
        "# create directories\n",
        "train_path=create_dir(DATASET_PATH,\"train\")\n",
        "test_path=create_dir(DATASET_PATH,\"test\")\n",
        "# image path\n",
        "train_img_path=create_dir(train_path,\"images\")\n",
        "test_img_path=create_dir(test_path,\"images\")\n",
        "# data.txt\n",
        "train_data_txt=os.path.join(train_path,\"data.txt\")\n",
        "test_data_txt=os.path.join(test_path,\"data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copy_paths(df,img_path):\n",
        "    _paths=df[\"filename\"].tolist()\n",
        "    for _path in tqdm(_paths):\n",
        "        base=os.path.basename(_path)\n",
        "        dst=os.path.join(img_path,base)\n",
        "        copy(_path,dst)\n",
        "\n",
        "def create_data_txt(df,data_txt):\n",
        "    with open(data_txt,\"w+\") as f:\n",
        "        for idx in tqdm(range(len(df))):\n",
        "            _path=df.iloc[idx,0]\n",
        "            _label=df.iloc[idx,1]\n",
        "            f.write(f\"{_path}\\t{_label}\\n\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1306/1306 [00:00<00:00, 15639.98it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 15346.96it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 25599.59it/s]\n",
            "100%|██████████| 1306/1306 [00:00<00:00, 30254.01it/s]\n"
          ]
        }
      ],
      "source": [
        "train=df.loc[df.fold==\"train\"]\n",
        "test=df.loc[df.fold==\"test\"]\n",
        "\n",
        "train=train[[\"filename\",\"SN\"]]\n",
        "test=test[[\"filename\",\"SN\"]]\n",
        "copy_paths(test,test_img_path)\n",
        "copy_paths(train,train_img_path)\n",
        "create_data_txt(train,train_data_txt)\n",
        "create_data_txt(test,test_data_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
